{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Climate Change Visualization Notebook\n",
    "## Goal\n",
    "The goal of this project is to visualize temperature change on Earth over a significant time period of at least 500 years. \n",
    "I chose this project because climate change, as a contemporary point of debate, is is a very important issue. Although I am staunchly of the belief that climate change is a serious issue, I recognize that there are people who do not share this sentiment or do not consider climate change legitimate. I believe an easily-digested visual representation of climate change, with as little bias as possible, will allow people to form their own opinions on the change, or lack there of, in the Earth's climate.\n",
    "## Characteristics of Desired Data\n",
    "For this project, I will need to collect data that spans (preferably) over 500 years.\n",
    "Through research, I have learned that there are several different techniques for estimating the average temperature for a certain year in the past. I hope to gather several data sets that are the results of different techniques.\n",
    "I want to gather historical temperature data for as long of a time period as possible while still being able to verify its legitimacy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering the Data\n",
    "### Changing from a Solar System to a Global visualization\n",
    "I initially wanted to present an animated visualization comparing the variance of atmospheric gases, temperature, or some other metric of climate change of Earth to other planets in the solar system. I liked this idea because it provided the other planets as independent variables to compare Earth to. \n",
    "\n",
    "I found [this visualization](https://codepen.io/juliangarnier/pen/idhuG/) which I thought would be perfect for showcasing this data. The graphics had been mostly completed for me; it even has a label showcasing one of Earth's traits. I planned to speed up the revolutions of the planets to show hundreds, or thousands, of years in roughly a minute. I also planned to add an auxiliary time-series chart below the solar system that would be drawn as the planets rotated.\n",
    "\n",
    "However, after thirty minutes of research for climate data from other planets, I could not find a reputable data source that did not clearly label their data as rough estimates. I believe accurate historical (for the time range I desired) climate change data did not exist.&ast; It seemed unfair to compare the fairly accurate historical data for Earth to the inaccurate estimates for other planets.\n",
    "\n",
    "&ast;I contemplated using data from the past ten or so years, which was available. I believed this was not a meaningful enough time period for a visualization relating to how planets change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Finding the CDIAC datasets\n",
    "After filtering through several short articles and papers that only included graphics, I found a [repository of historical climate information](http://cdiac.ess-dive.lbl.gov/climate/temp/temp_table.html) that seemed fairly official. I googled the organization that claimed ownership for the data, the [CDIAC](https://en.wikipedia.org/wiki/Carbon_Dioxide_Information_Analysis_Center), or Carbon Dioxide Information Analysis Center, to find that they were a United States Department of Energy organization that ran from 1982-September 30, 2017.\n",
    "\n",
    "### Finding an API for CDIAC data\n",
    "I found a [dataset](http://cdiac.ess-dive.lbl.gov/ftp/trends/temp/vostok/vostok.1999.temp.dat) about the Vostok Ice Core that spans over 422,767 years. This dataset seemed to be in a .dat format, which I researched. While researching, it occured to me that this information might be readily available through an API. I could not easily find a CDIAC API. However, it appeared that they had collaborated with [NOAA](http://www.noaa.gov/),the National Oceanic and Atmospheric Administration in collecting this data. I quickly found an [NOAA API](https://www.ncdc.noaa.gov/cdo-web/webservices/v2).\n",
    "\n",
    "After having significant trouble simply loading the datasets using the NOAA API, I decided to search the error I was getting (`Token parameter is required.`). I quickly found [this blog post](http://emilkirkegaard.dk/en/?p=5904) which described the exact same issue. I realized I had not been placing by URL in quotes and I also had simply been stating my token with a `token:` indicator in front of it. I had used this format for the command:\n",
    "`curl https://www.ncdc.noaa.gov/cdo-web/api/v2/datasets 123456`\n",
    "instead of with the proper format:\n",
    "`curl 'https://www.ncdc.noaa.gov/cdo-web/api/v2/datasets' -H 'token: 123456'`.\n",
    "The printout from the `curl` is shown below using Python's `requests` library.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "url = 'https://www.ncdc.noaa.gov/cdo-web/api/v2/datasets'\n",
    "# I learned how to use a token with requests at:\n",
    "# http://docs.python-requests.org/en/master/user/quickstart/\n",
    "headers = {'token': 'your_token'}\n",
    "r = requests.get(url, headers=headers)\n",
    "# r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Berkeley Data\n",
    "After encountering some challenges turning the CDIAC data into a usable format such as CSV or GeoJSON, I decided to search again for usable data. I quickly found [Berkeley Earth](http://berkeleyearth.org/) which makes [its data](http://berkeleyearth.org/source-files/) readily available in a format that is easily turned into a CSV.\n",
    "\n",
    "I found a liking for this data because although it did not cover the date range for which I had hoped, it was gathered by expert scientists and is very detailed.\n",
    "\n",
    "I turned each `data.txt` file into a CSV by:\n",
    "1. Opening `data.txt` in Excel\n",
    "2. Deleting the textual rows before the data started\n",
    "3. Deleting the \"Series Number\", \"Uncertainty (C)\", \"Observations\", and \"Time of Observation\" columns. For each of these columns, the data values were identical in every row and not relevant to the visualization.\n",
    "4. Adding labels to each column. (`station_id`, `date`, and `temperature_c`)\n",
    "5. Saving the file as a CSV.\n",
    "\n",
    "### Creating a JSON Object from the CSV\n",
    "I used [this Stack Overflow page](https://stackoverflow.com/questions/19697846/python-csv-to-json) for guidance.\n",
    "\n",
    "Also https://courses.cs.washington.edu/courses/cse140/13wi/csv-parsing.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import pandas as pd\n",
    "from math import isnan\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createDataDict(csvVar, siteDetailFileCSV):\n",
    "    reader = csv.DictReader(csvVar)\n",
    "    data = {}\n",
    "    yearIterator = {}\n",
    "    for row in reader:\n",
    "        data = {row['station_id']: {row['date'][:4]:float(row['temperature_c'])}}\n",
    "        yearIterator = {'id':row['station_id'],'year':row['date'][:4],'sum':1}\n",
    "        break\n",
    "\n",
    "    for row in reader:\n",
    "#         get the year from the first four characters of row['date']\n",
    "        year = row['date'][:4]\n",
    "\n",
    "#       if changing to a new station_id\n",
    "        if not row['station_id'] in data:\n",
    "            data[yearIterator['id']][yearIterator['year']] /= yearIterator['sum']\n",
    "\n",
    "            data[row['station_id']] = {year: float(row['temperature_c'])}\n",
    "            yearIterator = {'id':row['station_id'],'year':year, 'sum':1}\n",
    "#       if changing to a new year with the same station_id\n",
    "        else:\n",
    "            if yearIterator['year'] != year:\n",
    "                data[yearIterator['id']][yearIterator['year']] /= yearIterator['sum']\n",
    "\n",
    "                data[row['station_id']][year] = float(row['temperature_c'])\n",
    "                yearIterator = {'id':row['station_id'],'year':year, 'sum':1}\n",
    "#           if iterating in the same station and the same year\n",
    "            else:\n",
    "                data[row['station_id']][year] += float(row['temperature_c'])\n",
    "                yearIterator['sum'] += 1\n",
    "    \n",
    "    site_df = pd.read_csv(siteDetailFileCSV)\n",
    "    if len(site_df.columns) > 5:\n",
    "        site_df.drop(site_df.columns[[list(range(6,len(site_df.columns)))]],axis=1,inplace=True)\n",
    "    site_df['station_name']=site_df['station_name'].str.strip()\n",
    "    site_df = site_df.loc[site_df['latitude'] != -999.00]\n",
    "\n",
    "    for i in list(data.keys()):\n",
    "        if float(i) not in list(site_df['station_id']):\n",
    "            del data[i]\n",
    "        else:\n",
    "            temp = data[i]\n",
    "            del data[i]\n",
    "            data[i] = {}\n",
    "            data[i]['years'] = temp\n",
    "\n",
    "    for index, row in site_df.iterrows():\n",
    "        if str(row['station_id']) in data.keys():\n",
    "            data[str(row['station_id'])]['station_name'] = row['station_name']\n",
    "            data[str(row['station_id'])]['latitude'] = row['latitude']\n",
    "            data[str(row['station_id'])]['longitude'] = row['longitude']\n",
    "            data[str(row['station_id'])]['elevation_m'] = row['elevation_m']\n",
    "\n",
    "    d = [{'station_id':key,\"data\":value} for key,value in data.items()] \n",
    "    return d\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_names = [{\"data\": \"colonial\", \"site_detail\": \"colonial_site_detail\", \"outfile\": \"colonialdata\"},\n",
    "            {\"data\": \"monthly_climatic_data\", \"site_detail\":\"monthly_climatic_data_site_detail\", \"outfile\": \"monthlyclimaticdata\"},\n",
    "            {\"data\": \"ghcn_daily\", \"site_detail\":\"ghcn_site_detail\", \"outfile\": \"ghcndata\"},\n",
    "            {\"data\":\"world_monthly_surface_station_climatology\", \"site_detail\":\"world_monthly_surface_station_climatology_site_detail\", \"outfile\":\"wmssc\"}]\n",
    "\n",
    "for i in csv_names:\n",
    "    dcf = \"data_csv_files/\"\n",
    "    ccd = \"cleaned_climate_data/\"\n",
    "    data_csv = open(dcf + i[\"data\"] + \".csv\", 'r')\n",
    "    with open(ccd + i[\"outfile\"] + '.json', 'w') as outfile:  \n",
    "        json.dump(createDataDict(data_csv,dcf + i['site_detail'] + '.csv'), outfile)\n",
    "# colonial = open('colonial.csv', 'r')\n",
    "# with open('colonialdata.json', 'w') as outfile:  \n",
    "#     json.dump(createDataDict(colonial,'colonial_site_detail.csv'), outfile)\n",
    "    \n",
    "# mcd = open('monthly_climatic_data.csv')\n",
    "# with open('monthlyclimaticdata.json', 'w') as outfile:  \n",
    "#     json.dump(createDataDict(mcd,'monthly_climatic_data_site_detail.csv'), outfile)\n",
    "    \n",
    "# ghcn = open('ghcn_daily.csv', 'r')\n",
    "# with open('ghcndata.json', 'w') as outfile:  \n",
    "#     json.dump(createDataDict(ghcn,'ghcn_site_detail.csv'), outfile)\n",
    "    \n",
    "# wmssc = open('world_monthly_surface_station_climatology.csv', 'r')\n",
    "# with open ('wmssc.json', 'w') as outfile:\n",
    "#     json.dump(createDataDict(wmssc,'world_monthly_surface_station_climatology_site_detail.csv'), outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making the Code Above\n",
    "I went through some trouble in writing the code that made the surfacedata.json file small enough. I particularly had some trouble in averaging the temperatures for each year.\n",
    "### Connecting Stations To Physical Locations\n",
    "To show the locations on a physical map, I realized I had to connect the stations from `data.txt` to the corrolary stations in `site_detail.txt`. To do this, I cleaned `site_detail.txt` in a similar fashion as `data.txt` in Excel. For my purposes, the following values for each station were not relevant:\n",
    "* Latitude Uncertainty\n",
    "* Longitude Uncertainty\n",
    "* Elevation Uncertainty\n",
    "* State/ Province Code\n",
    "* County\n",
    "* Time Zone\n",
    "* WMO ID\n",
    "* Coop ID\n",
    "* WBAN ID,\n",
    "* ICAO ID\n",
    "* Number of Relocations\n",
    "* Number of Suggested Relocations\n",
    "* Number of Sources\n",
    "* Hash\n",
    "\n",
    "I chose to only keep the Station ID, Station Name, Latitude, Longitude, Elevation, and Country. I now need to create a JSON object that connects each station to its information provided in the `site_detail.txt` file. I will also filter out the stations where the latitude and longitude are not provided, as they cannot be dynamically located on a global map.\n",
    "\n",
    "https://stackoverflow.com/questions/41998624/how-to-convert-pandas-dataframe-to-nested-dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
